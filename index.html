<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Peculiarities of Mixture-of-Expert optimization</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Peculiarities of Mixture-of-Expert optimization</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://jyopari.github.io" target="_blank">Jyothish Pari</a><sup>*</sup>,</span>
                <span class="author-block">
                  <a href="https://https://minyounghuh.com/" target="_blank">Minyoung Huh</a><sup>*</sup>,</span>
                  <span class="author-block">
                    <a href="https://people.csail.mit.edu/pulkitag/" target="_blank">Pulkit Agrawal</a>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Massachusetts Institute of Technology<br></span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
<!--                       <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span> -->

                    <!-- Supplementary PDF link -->
<!--                     <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section hero is-small is-dark">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3">Method Overview</h2>
          <center>
          <img src="static/images/rpo.png" alt="Mixed Video-Image Finetuning" class="center-image blend-img-background"/>
          </center>
          <div class="level-set has-text-justified">
            <p>
              We present Router Policy Optimization (RPO), a novel training mechanism for MoE within transformer models, structured in two distinct phases: router policy optimization (blue) and expert optimization (red). In the router policy optimization phase, input tokens are directed through a router policy, which determines the subset of experts to utilize. The output from these selected experts is then weighted and aggregated. The accompanying pseudocode details our alternating training algorithm, where the expectation step updates the router policy using reinforcement learning, focusing solely on router parameters. The maximization step updates the rest of the model parameters, excluding the routersâ€™. By applying this alternative EM approach, RPO adeptly addresses the intricacies of expert collapse and the balancing of expert contributions, leading to more precise and efficient routing that significantly improves model performance.            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <!-- <source src="static/videos/banner_video.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus. 
      </h2>
    </div>
  </div>
</section> --> 
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Mixture-of-Experts (MoE) has proven to be a pivotal technique for reducing computational demands while maintaining scaling behaviors. Yet, their optimization
intricacies and potential pitfalls remain under-explored. In this paper, we investigate challenges with router optimization in MoE, particularly within the context
of transformers. We formalize the MoE training objective and use it to dissect
the operational nuances of these models. We highlight key challenges including
expert collapse, where experts converge to similar representations; monotonicity
barriers, a requirement for a smooth linear combination of the experts; expert
weighting dilemma, a dichotomy between the expert choice probability and expert
weighting; and routerless-routing, where expert/attention mechanisms can learn
to route even without explicit router training. To counteract these challenges, we
present Router-Policy Optimization (RPO), a novel approach that combines
expectation-maximization and policy optimization. We conduct a comprehensive
analysis of MoE models under various scenarios to identify when these problems
can occur and propose strategies to overcome them.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Image carousel -->
<section class="section hero is-small is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3">Router Optimization Visualization</h2>
          <center>
          <img src="static/images/2d_viz-modified (3).png" alt="Router Optimization Visualization" class="center-image blend-img-background"/>
          </center>
          <div class="level-set has-text-justified">
            <p>
              We demonstrate the challenges in router optimization. In a toy scenario, experts are represented by vectors in a regression problem aimed at minimizing the distance to a red star target. The routing trajectory is visualized, contrasting the different methodsâ€™ performance. RPO successfully identifies the most effective routing, while standard optimization and variants falls short, especially evident when more than one expert is involved.           </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End image carousel -->


<section class="section hero is-small is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3">Expert Collapse</h2>
          <center>
          <img src="static/images/expert_collapse.pnd-modified.png" alt="Router Optimization Visualization" class="center-image blend-img-background" width="600" />
          </center>
          <div class="level-set has-text-justified">
            <p>
              We capture the phenomenon of expert collapse in MoE models. When routers and experts are optimized together in a conventional manner, the experts tend to converge towards eachother in their outputs. This concept is illustrated by starting with experts at optimal positions in a two-dimensional space, marked by a dashed line and circle. Over the course of training, we observe that standard optimization tends to merge the distinct expert representations, as shown by the transition from lighter to darker shades. In contrast, Router Policy Optimization (RPO) maintains the diversity of the experts, ensuring they remain true to the initial optimal assignments. We observe the effect of weight decay (wd) which prevents the norm of the router weight matrix from exploding.         </div>
      </div>
    </div>
  </div>
</section>


<section class="section hero is-small is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3">GPT2 Optimal Routing Recovery</h2>
          <center>
          <img src="static/images/model_diff_sim_v2-modified.png" alt="Router Optimization Visualization" class="center-image blend-img-background" width="900" />
          <img src="static/images/model_diff_sim_losses-modified.png" alt="Router Optimization Visualization" class="center-image blend-img-background" width="1000" />
          </center>
          <div class="level-set has-text-justified">
            <p>
              Here experts are fixed. We initialize a MoE model using GPT2, pre-trained on Openwebtext, comprising 4 experts. One expert is initialized with pre-trained weights, while the remaining three are set as sub-optimal experts. We only optimize for the router to highlight the failure to recover the optimal routing. On the left, the three suboptimal experts are started with random weights (Model DIFF) on the right, they are initialized with perturbed optimal weights (Model SIM). When the weights are randomly initialized, an unnormalized MoE arrives at the correct routing. However, when sub-optimal experts can partially solve the problem, standard training fails and only Router Policy Optimization (RPO) is capable of accurately of recovering the correct solution.
            </div>
    </div>
  </div>
</section>









<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from theÂ <a href="https://nerfies.github.io" target="_blank">Nerfies</a>Â project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
